{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465d6dc6e32c46bcae4a96b50a3111d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import warnings \n",
    "from dial2vec_mlx.model import DialogueTransformer\n",
    "from dial2vec.data import get_sessions \n",
    "\n",
    "from mlx_embeddings import load\n",
    "import mlx.nn as nn \n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim \n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "\n",
    "# Some libraries use deprecated functions, ignore to prevent spam. \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "# get from https://drive.google.com/file/d/1KpxQGXg9gvH-2u21bAMykL5N-tpYU2Dr/view?usp=sharing\n",
    "training_file = \"datasets/doc2dial/train.tsv\"\n",
    "testing_file = \"datasets/doc2dial/clustering_test.tsv\"\n",
    "\n",
    "model_config = {\n",
    "    # Max amount of tokens, anything above or below is pruned. \n",
    "    \"max_tokens\" : 386,     # config.max_position_embeddings\n",
    "    # Temperature for cosine distancing \n",
    "    \"temperature\" : 1.0, \n",
    "    # How many samples to use for training \n",
    "    \"batch_train_size\": 5, \n",
    "    # How many samples to use for testing \n",
    "    \"batch_test_size\" : 5,\n",
    "    # How many layers should be frozen in BERT variants \n",
    "    # Paper specifies that it freezes bottom 6 (out of 12) so we get something close to the ~50%\n",
    "    \"freeze_upto\" : 14, \n",
    "    \"learning_rate\" : 1e-5,\n",
    "    \"gradient_accumulation_steps\" : 1.0 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_sessions(training_file, tokenizer, model.config, **model_config)\n",
    "clustering_test = get_sessions(testing_file, tokenizer, model.config, **model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "\n",
    "for i in range(model_config[\"freeze_upto\"]):\n",
    "    model.modules()[0][\"model\"][\"layers\"][i].freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "# usage of mlx.data overloads ram and crashes OS on my experience \n",
    "random.shuffle(features)\n",
    "\n",
    "# Don't need turn ids or segment ids in modernBERT AFAIK \n",
    "all_input_ids = mx.array([f.input_ids for f in features], dtype = mx.int64)\n",
    "all_input_mask = mx.array([f.input_mask for f in features], dtype = mx.int64)\n",
    "all_position_ids = mx.array([f.position_ids for f in features], dtype = mx.int64)\n",
    "all_role_ids = mx.array([f.role_ids for f in features], dtype = mx.int64)\n",
    "all_labels = mx.array([f.label_id for f in features],dtype = mx.int64)\n",
    "\n",
    "sample_num = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://huggingface.co/transformers/v1.0.0/migration.html\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/df99f8c5a1c54d64fb013b43107011390c3be0d5/transformers/optimization.py#L45\n",
    "# https://github.com/pytorch/pytorch/blob/main/torch/optim/lr_scheduler.py\n",
    "def linear_warmup_with_decay(step, warmup_steps, total_steps, peak_lr, min_lr = 0.0):\n",
    "    if step < warmup_steps:\n",
    "            return peak_lr * (step / warmup_steps)\n",
    "    else: \n",
    "          decay_steps = total_steps - warmup_steps\n",
    "          decay_ratio = (total_steps - step) / decay_steps\n",
    "          return min_lr + (peak_lr - min_lr) * decay_ratio\n",
    "    \n",
    "def create_scheduler(peak_lr, warmup_steps, total_steps):\n",
    "      def scheduler(step):\n",
    "            return linear_warmup_with_decay(step, warmup_steps, total_steps, peak_lr)\n",
    "      return scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DialogueTransformer.__init__() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m average\n\u001b[32m      4\u001b[39m plotlosses = PlotLosses()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dial2vec = \u001b[43mDialogueTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m mx.eval(dial2vec.parameters())\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_fn\u001b[39m(model, inputs): \n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Not sure if legit but gradients need to be computed inside of value_and_grad otherwise won't flow down layers fully \u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: DialogueTransformer.__init__() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from numpy import average\n",
    "\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "dial2vec = DialogueTransformer(model, model_config)\n",
    "mx.eval(dial2vec.parameters())\n",
    "\n",
    "def loss_fn(model, inputs): \n",
    "    # Not sure if legit but gradients need to be computed inside of value_and_grad otherwise won't flow down layers fully \n",
    "    out = dial2vec(*inputs)\n",
    "    return out['loss'] \n",
    "\n",
    "\n",
    "loss_and_grad_fn = nn.value_and_grad(dial2vec, loss_fn)\n",
    "optimizer = optim.AdamW(learning_rate = model_config['learning_rate'], eps = 1e-6)\n",
    "\n",
    "epochs = 5\n",
    "batch = model_config[\"batch_train_size\"]\n",
    "full_run = sample_num * epochs / batch  \n",
    "\n",
    "scheduler = create_scheduler(model_config['learning_rate'], warmup_steps = int(full_run * 0.1), total_steps = full_run)\n",
    "\n",
    "step = 0\n",
    "losses = [] \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i in range(0, sample_num, batch):\n",
    "        if sample_num - i == 0: \n",
    "            continue \n",
    "\n",
    "        \n",
    "        lr = scheduler(step)\n",
    "        optimizer.learning_rate = lr  \n",
    "\n",
    "\n",
    "        inputs = (\n",
    "            all_input_ids[i : i + batch, :, :],\n",
    "            all_input_mask[i : i + batch, :, :],\n",
    "            all_position_ids[i : i + batch, :, :],\n",
    "            all_role_ids[i : i + batch, :, :],\n",
    "            all_labels[i : i + batch]\n",
    "        )\n",
    "\n",
    "        loss, grads = loss_and_grad_fn(dial2vec, inputs)\n",
    "        optimizer.update(dial2vec, grads)\n",
    "        mx.eval(dial2vec.parameters(), optimizer.state, loss)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            plotlosses.update({'loss' : average(losses)})\n",
    "            plotlosses.send()     \n",
    "            losses = [] \n",
    "\n",
    "        step += 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dial2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
