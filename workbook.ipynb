{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/26/2025 10:45:24 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import logging \n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = \"mps\") \n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "start_token, sep_token, pad_token_id = tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token_id\n",
    "\n",
    "import os \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_statistics(file_name):\n",
    "    \"\"\"\n",
    "    统计文件行数\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        return 0\n",
    "\n",
    "    return 6948 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "\n",
    "class PoolingAverage(nn.Module):\n",
    "    def __init__(self, eps=1e-12):\n",
    "        super(PoolingAverage, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = lambda x, m: x * torch.unsqueeze(m, dim=-1)\n",
    "        reduce_mean = lambda x, m: torch.sum(mul_mask(x, m), dim=1) / (torch.sum(m, dim=1, keepdims=True) + self.eps)\n",
    "\n",
    "        avg_output = reduce_mean(hidden_states, attention_mask)\n",
    "        return avg_output\n",
    "\n",
    "    def equal_forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = hidden_states * attention_mask.unsqueeze(-1)\n",
    "        avg_output = torch.sum(mul_mask, dim=1) / (torch.sum(attention_mask, dim=1, keepdim=True) + self.eps)\n",
    "        return avg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DialogueTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DialogueTransformer, self).__init__()\n",
    "        self.bert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = \"mps\") \n",
    "\n",
    "        hf_model = model._first_module().auto_model  # or model[0].auto_model\n",
    "        self.config = hf_model.config\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.labels_data = None\n",
    "        self.sample_nums = 10\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.avg = PoolingAverage(eps=1e-6)\n",
    "        self.logger = logger  \n",
    "\n",
    "    def forward(self, data, strategy='mean_by_role', output_attention=False):\n",
    "\n",
    "\n",
    "        if len(data) == 7:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels = data\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels, guids = data\n",
    "\n",
    "        input_ids = input_ids.view(input_ids.size()[0] * input_ids.size()[1], input_ids.size()[-1])\n",
    "        attention_mask = attention_mask.view(attention_mask.size()[0] * attention_mask.size()[1], attention_mask.size()[-1])\n",
    "        token_type_ids = token_type_ids.view(token_type_ids.size()[0] * token_type_ids.size()[1], token_type_ids.size()[-1])\n",
    "        role_ids = role_ids.view(role_ids.size()[0] * role_ids.size()[1], role_ids.size()[-1])\n",
    "        turn_ids = turn_ids.view(turn_ids.size()[0] * turn_ids.size()[1], turn_ids.size()[-1])\n",
    "        position_ids = position_ids.view(position_ids.size()[0] * position_ids.size()[1], position_ids.size()[-1])\n",
    "\n",
    "        one_mask = torch.ones_like(role_ids)\n",
    "        zero_mask = torch.zeros_like(role_ids)\n",
    "        role_a_mask = torch.where(role_ids == 0, one_mask, zero_mask)\n",
    "        role_b_mask = torch.where(role_ids == 1, one_mask, zero_mask)\n",
    "\n",
    "        sep_token_id = self.bert.tokenizer.sep_token_id \n",
    "        a_attention_mask = (attention_mask * role_a_mask)\n",
    "        b_attention_mask = (attention_mask * role_b_mask)\n",
    "\n",
    "        self_output, pooled_output = self.encoder(input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids)\n",
    "\n",
    "        q_self_output = self_output * a_attention_mask.unsqueeze(-1)\n",
    "        r_self_output = self_output * b_attention_mask.unsqueeze(-1)\n",
    "\n",
    "        self_output = self_output * attention_mask.unsqueeze(-1)\n",
    "        w = torch.matmul(q_self_output, r_self_output.transpose(-1, -2))\n",
    "\n",
    "        if turn_ids is not None:\n",
    "            view_turn_mask = turn_ids.unsqueeze(1).repeat(1, self.bert.max_seq_length, 1)\n",
    "            view_turn_mask_transpose = view_turn_mask.transpose(2, 1)\n",
    "            view_range_mask = torch.where(abs(view_turn_mask_transpose - view_turn_mask) <= 1000,\n",
    "                                          torch.ones_like(view_turn_mask),\n",
    "                                          torch.zeros_like(view_turn_mask))\n",
    "            filtered_w = w * view_range_mask\n",
    "\n",
    "        q_cross_output = torch.matmul(filtered_w.permute(0, 2, 1), q_self_output)\n",
    "        r_cross_output = torch.matmul(filtered_w, r_self_output)\n",
    "\n",
    "        q_self_output = self.avg(q_self_output, a_attention_mask)\n",
    "        q_cross_output = self.avg(q_cross_output, b_attention_mask)\n",
    "        r_self_output = self.avg(r_self_output, b_attention_mask)\n",
    "        r_cross_output = self.avg(r_cross_output, a_attention_mask)\n",
    "\n",
    "        self_output = self.avg(self_output, attention_mask)\n",
    "        q_self_output = q_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        q_cross_output = q_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_self_output = r_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_cross_output = r_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        self_output = self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        pooled_output = pooled_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        output = self_output[:, 0, :]\n",
    "        q_output = q_self_output[:, 0, :]\n",
    "        r_output = r_self_output[:, 0, :]\n",
    "        q_contrastive_output = q_cross_output[:, 0, :]\n",
    "        r_contrastive_output = r_cross_output[:, 0, :]\n",
    "\n",
    "        logit_q = []\n",
    "        logit_r = []\n",
    "        for i in range(self.sample_nums):\n",
    "            cos_q = self.calc_cos(q_self_output[:, i, :], q_cross_output[:, i, :])\n",
    "            cos_r = self.calc_cos(r_self_output[:, i, :], r_cross_output[:, i, :])\n",
    "            logit_r.append(cos_r)\n",
    "            logit_q.append(cos_q)\n",
    "\n",
    "        logit_r = torch.stack(logit_r, dim=1)\n",
    "        logit_q = torch.stack(logit_q, dim=1)\n",
    "\n",
    "        loss_r = self.calc_loss(logit_r, labels)\n",
    "        loss_q = self.calc_loss(logit_q, labels)\n",
    "\n",
    "        if strategy not in ['mean', 'mean_by_role']:\n",
    "            raise ValueError('Unknown strategy: [%s]' % strategy)\n",
    "\n",
    "        output_dict = {'loss': loss_r + loss_q,\n",
    "                       'final_feature': output if strategy == 'mean' else q_output + r_output,\n",
    "                       'q_feature': q_output,\n",
    "                       'r_feature': r_output,\n",
    "                       'attention': w}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def encoder(self, *x):\n",
    "        input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids = x    \n",
    "\n",
    "        transformer = self.bert[0].auto_model\n",
    "        output = transformer(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        all_output = output['hidden_states']\n",
    "        pooler_output = output['pooler_output']\n",
    "        return all_output[-1], pooler_output\n",
    "\n",
    "    def calc_cos(self, x, y):\n",
    "        cos = torch.cosine_similarity(x, y, dim=1)\n",
    "        cos = cos / 1.0 # cos = cos / 2.0\n",
    "        return cos\n",
    "\n",
    "    def calc_loss(self, pred, labels):\n",
    "        loss = -torch.mean(self.log_softmax(pred) * labels)\n",
    "        return loss\n",
    "\n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "\n",
    "    def get_labels_data(self):\n",
    "        return self.labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "class BertFeatures():\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, role_ids, label_id, turn_ids=None, position_ids=None, guid=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.role_ids = role_ids\n",
    "        self.turn_ids = turn_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.label_id = label_id\n",
    "        self.guid = guid\n",
    "\n",
    "        self.batch_size = len(self.input_ids)\n",
    "import codecs  \n",
    "\n",
    "features = [] \n",
    "\n",
    "with codecs.open(\"datasets/doc2dial/train.tsv\", \"r\", \"utf\") as f: \n",
    "    examples = [] \n",
    "    for line in f: \n",
    "        line = [s.strip() for s in line.split('\\t') if s.strip()]\n",
    "        role, session, label = line[0], line[1], line[2]\n",
    "\n",
    "        examples.append((role, session, label))\n",
    "\n",
    "    for example in examples: \n",
    "        samples = example[1].split(\"|\")\n",
    "        roles = [int(r) for r in example[0].split(\"|\")] \\\n",
    "                if example[0].find(\"#\") != -1 \\\n",
    "                else [int(r) for r in example[0]]\n",
    "    \n",
    "    \n",
    "        sample_input_ids = []\n",
    "        sample_segment_ids = []\n",
    "        sample_role_ids = []\n",
    "        sample_input_mask = []\n",
    "        sample_turn_ids = []\n",
    "        sample_position_ids = []\n",
    "\n",
    "        for t, s in enumerate(samples):\n",
    "            text_tokens = []\n",
    "            text_turn_ids = []\n",
    "            text_role_ids = []\n",
    "\n",
    "            texts = s.split(\"#\")\n",
    "\n",
    "            # bert-token:     [cls]  token   [sep]  token\n",
    "            # roberta-token:   <s>   token   </s>   </s> token\n",
    "\n",
    "\n",
    "            text_tokens.append(start_token)\n",
    "            text_turn_ids.append(0)\n",
    "            text_role_ids.append(roles[0])\n",
    "\n",
    "            for i, text in enumerate(texts): \n",
    "\n",
    "                 tokenized = tokenizer.tokenize(text)\n",
    "                 text_tokens.extend(tokenized)\n",
    "                 text_turn_ids.extend([i] * len(tokenized))\n",
    "                 text_role_ids.extend([roles[i]] * len(tokenized))\n",
    "\n",
    "                 if i != (len(text) - 1): \n",
    "                     text_tokens.append(sep_token)\n",
    "                     text_turn_ids.append(i)\n",
    "                     text_role_ids.append(roles[i])\n",
    "\n",
    "            text_tokens = text_tokens[:model.max_seq_length]\n",
    "            text_turn_ids = text_turn_ids[:model.max_seq_length]\n",
    "            text_role_ids = text_role_ids[:model.max_seq_length]\n",
    "\n",
    "            text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "\n",
    "\n",
    "            text_input_ids += [pad_token_id] * (model.max_seq_length - len(text_tokens))\n",
    "            text_input_mask = [1] * len(text_tokens) + [0] * (model.max_seq_length - len(text_tokens))\n",
    "            text_segment_ids = [0] * model.max_seq_length\n",
    "            text_position_ids = list(range(len(text_tokens))) + [0] * (model.max_seq_length - len(text_tokens))\n",
    "            text_turn_ids += [0] * (model.max_seq_length - len(text_tokens))\n",
    "            text_role_ids += [0] * (model.max_seq_length - len(text_tokens))\n",
    "\n",
    "\n",
    "            assert len(text_input_ids) == model.max_seq_length\n",
    "            assert len(text_input_mask) == model.max_seq_length\n",
    "            assert len(text_segment_ids) == model.max_seq_length\n",
    "            assert len(text_position_ids) == model.max_seq_length\n",
    "            assert len(text_turn_ids) == model.max_seq_length\n",
    "            assert len(text_role_ids) == model.max_seq_length\n",
    "\n",
    "            sample_input_ids.append(text_input_ids)\n",
    "            sample_turn_ids.append(text_turn_ids)\n",
    "            sample_role_ids.append(text_role_ids)\n",
    "            sample_segment_ids.append(text_segment_ids)\n",
    "            sample_position_ids.append(text_position_ids)\n",
    "            sample_input_mask.append(text_input_mask) \n",
    "\n",
    "\n",
    "        n_neg = 9\n",
    "        label_id = [1] + [0] * n_neg\n",
    "        bert_feature = BertFeatures(input_ids=sample_input_ids,\n",
    "                                    input_mask=sample_input_mask,\n",
    "                                    segment_ids=sample_segment_ids,\n",
    "                                    role_ids=sample_role_ids,\n",
    "                                    turn_ids=sample_turn_ids,\n",
    "                                    position_ids=sample_position_ids,\n",
    "                                    label_id=label_id,\n",
    "                                    guid=[None] * (1 + n_neg))\n",
    "\n",
    "        features.append(bert_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\n",
    "import torch \n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_role_ids = torch.tensor([f.role_ids for f in features], dtype=torch.long)\n",
    "all_turn_ids = torch.tensor([f.turn_ids for f in features], dtype=torch.long)\n",
    "all_position_ids = torch.tensor([f.position_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids,\n",
    "                            all_input_mask,\n",
    "                            all_segment_ids,\n",
    "                            all_role_ids,\n",
    "                            all_turn_ids,\n",
    "                            all_position_ids,\n",
    "                            all_label_ids)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data,\n",
    "                        sampler=train_sampler,\n",
    "                        batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/26/2025 10:47:14 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4607, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4600, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4607, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4602, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4608, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4604, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4600, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4602, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4604, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4602, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4607, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4601, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4601, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4599, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4602, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4604, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4598, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4598, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4598, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4601, device='mps:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.4602, device='mps:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m     23\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/optimization.py:147\u001b[39m, in \u001b[36mBERTAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Add grad clipping\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m'\u001b[39m\u001b[33mmax_grad_norm\u001b[39m\u001b[33m'\u001b[39m] > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_grad_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[32m    151\u001b[39m next_m.mul_(beta1).add_(grad, alpha=\u001b[32m1\u001b[39m - beta1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:220\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    219\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43m_clip_grads_with_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:176\u001b[39m, in \u001b[36m_clip_grads_with_norm_\u001b[39m\u001b[34m(parameters, max_norm, total_norm, foreach)\u001b[39m\n\u001b[32m    174\u001b[39m clip_coef_clamped_device = clip_coef_clamped.to(device)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from optimization import BERTAdam\n",
    "\n",
    "\n",
    "d2vmodel = DialogueTransformer().to(torch.device(\"mps\"))\n",
    "\n",
    "transformer = d2vmodel.bert[0].auto_model\n",
    "param_optimizer = list(transformer.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}]\n",
    "optimizer = BERTAdam(optimizer_grouped_parameters, lr=1e-5, warmup=0.1, t_total=line_statistics(\"datasets/doc2dial/train.tsv\"))\n",
    "\n",
    "steps = 0 \n",
    "d2vmodel.train()\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "        output_dict = d2vmodel(batch, strategy='mean_by_role')\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "        loss = loss / 1.0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if steps % 20 == 0: \n",
    "            print(\"Loss: \", loss.item())\n",
    "        if steps % 100 == 0:\n",
    "            d2vmodel.train()\n",
    "\n",
    "        steps += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dial2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
