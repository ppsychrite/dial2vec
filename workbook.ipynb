{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/croarkin.kyle/Desktop/dial2vec/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel \n",
    "import logging \n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = \"mps\") \n",
    "#tokenizer = model.tokenizer\n",
    "from metrics import * \n",
    "start_token, sep_token, pad_token_id = tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token_id\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import os \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_statistics(file_name):\n",
    "    \"\"\"\n",
    "    统计文件行数\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        return 0\n",
    "\n",
    "    return 6948 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "\n",
    "class PoolingAverage(nn.Module):\n",
    "    def __init__(self, eps=1e-12):\n",
    "        super(PoolingAverage, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = lambda x, m: x * torch.unsqueeze(m, dim=-1)\n",
    "        reduce_mean = lambda x, m: torch.sum(mul_mask(x, m), dim=1) / (torch.sum(m, dim=1, keepdims=True) + self.eps)\n",
    "\n",
    "        avg_output = reduce_mean(hidden_states, attention_mask)\n",
    "        return avg_output\n",
    "\n",
    "    def equal_forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = hidden_states * attention_mask.unsqueeze(-1)\n",
    "        avg_output = torch.sum(mul_mask, dim=1) / (torch.sum(attention_mask, dim=1, keepdim=True) + self.eps)\n",
    "        return avg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DialogueTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DialogueTransformer, self).__init__()\n",
    "        self.bert = model \n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.labels_data = None\n",
    "        self.sample_nums = 10\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.avg = PoolingAverage(eps=1e-6)\n",
    "        self.logger = logger  \n",
    "\n",
    "    def forward(self, data, strategy='mean_by_role', output_attention=False):\n",
    "\n",
    "\n",
    "        if len(data) == 7:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels = data\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels, guids = data\n",
    "\n",
    "        input_ids = input_ids.view(input_ids.size()[0] * input_ids.size()[1], input_ids.size()[-1])\n",
    "        attention_mask = attention_mask.view(attention_mask.size()[0] * attention_mask.size()[1], attention_mask.size()[-1])\n",
    "        token_type_ids = token_type_ids.view(token_type_ids.size()[0] * token_type_ids.size()[1], token_type_ids.size()[-1])\n",
    "        role_ids = role_ids.view(role_ids.size()[0] * role_ids.size()[1], role_ids.size()[-1])\n",
    "        turn_ids = turn_ids.view(turn_ids.size()[0] * turn_ids.size()[1], turn_ids.size()[-1])\n",
    "        position_ids = position_ids.view(position_ids.size()[0] * position_ids.size()[1], position_ids.size()[-1])\n",
    "\n",
    "        one_mask = torch.ones_like(role_ids)\n",
    "        zero_mask = torch.zeros_like(role_ids)\n",
    "        role_a_mask = torch.where(role_ids == 0, one_mask, zero_mask)\n",
    "        role_b_mask = torch.where(role_ids == 1, one_mask, zero_mask)\n",
    "\n",
    "        sep_token_id = tokenizer.sep_token_id \n",
    "        a_attention_mask = (attention_mask * role_a_mask)\n",
    "        b_attention_mask = (attention_mask * role_b_mask)\n",
    "\n",
    "        self_output, pooled_output = self.encoder(input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids)\n",
    "\n",
    "        q_self_output = self_output * a_attention_mask.unsqueeze(-1)\n",
    "        r_self_output = self_output * b_attention_mask.unsqueeze(-1)\n",
    "\n",
    "        self_output = self_output * attention_mask.unsqueeze(-1)\n",
    "        w = torch.matmul(q_self_output, r_self_output.transpose(-1, -2))\n",
    "\n",
    "        if turn_ids is not None:\n",
    "            view_turn_mask = turn_ids.unsqueeze(1).repeat(1, config.max_position_embeddings, 1)\n",
    "            view_turn_mask_transpose = view_turn_mask.transpose(2, 1)\n",
    "            view_range_mask = torch.where(abs(view_turn_mask_transpose - view_turn_mask) <= 1000,\n",
    "                                          torch.ones_like(view_turn_mask),\n",
    "                                          torch.zeros_like(view_turn_mask))\n",
    "            filtered_w = w * view_range_mask\n",
    "\n",
    "        q_cross_output = torch.matmul(filtered_w.permute(0, 2, 1), q_self_output)\n",
    "        r_cross_output = torch.matmul(filtered_w, r_self_output)\n",
    "\n",
    "        q_self_output = self.avg(q_self_output, a_attention_mask)\n",
    "        q_cross_output = self.avg(q_cross_output, b_attention_mask)\n",
    "        r_self_output = self.avg(r_self_output, b_attention_mask)\n",
    "        r_cross_output = self.avg(r_cross_output, a_attention_mask)\n",
    "\n",
    "        self_output = self.avg(self_output, attention_mask)\n",
    "        q_self_output = q_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        q_cross_output = q_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_self_output = r_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_cross_output = r_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        self_output = self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        pooled_output = pooled_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        output = self_output[:, 0, :]\n",
    "        q_output = q_self_output[:, 0, :]\n",
    "        r_output = r_self_output[:, 0, :]\n",
    "        q_contrastive_output = q_cross_output[:, 0, :]\n",
    "        r_contrastive_output = r_cross_output[:, 0, :]\n",
    "\n",
    "        logit_q = []\n",
    "        logit_r = []\n",
    "        for i in range(self.sample_nums):\n",
    "            cos_q = self.calc_cos(q_self_output[:, i, :], q_cross_output[:, i, :])\n",
    "            cos_r = self.calc_cos(r_self_output[:, i, :], r_cross_output[:, i, :])\n",
    "            logit_r.append(cos_r)\n",
    "            logit_q.append(cos_q)\n",
    "\n",
    "        logit_r = torch.stack(logit_r, dim=1)\n",
    "        logit_q = torch.stack(logit_q, dim=1)\n",
    "\n",
    "        loss_r = self.calc_loss(logit_r, labels)\n",
    "        loss_q = self.calc_loss(logit_q, labels)\n",
    "\n",
    "        if strategy not in ['mean', 'mean_by_role']:\n",
    "            raise ValueError('Unknown strategy: [%s]' % strategy)\n",
    "\n",
    "        output_dict = {'loss': loss_r + loss_q,\n",
    "                       'final_feature': output if strategy == 'mean' else q_output + r_output,\n",
    "                       'q_feature': q_output,\n",
    "                       'r_feature': r_output,\n",
    "                       'attention': w}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def encoder(self, *x):\n",
    "        input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids = x    \n",
    "\n",
    "        output = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        all_output = output['hidden_states']\n",
    "        pooler_output = output['pooler_output']\n",
    "        return all_output[-1], pooler_output\n",
    "\n",
    "    def calc_cos(self, x, y):\n",
    "        cos = torch.cosine_similarity(x, y, dim=1)\n",
    "        cos = cos / 1.0 # cos = cos / 2.0\n",
    "        return cos\n",
    "\n",
    "    def calc_loss(self, pred, labels):\n",
    "        loss = -torch.mean(self.log_softmax(pred) * labels)\n",
    "        return loss\n",
    "\n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "\n",
    "    def get_labels_data(self):\n",
    "        return self.labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFeatures():\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, role_ids, label_id, turn_ids=None, position_ids=None, guid=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.role_ids = role_ids\n",
    "        self.turn_ids = turn_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.label_id = label_id\n",
    "        self.guid = guid\n",
    "\n",
    "        self.batch_size = len(self.input_ids)\n",
    "import codecs  \n",
    "\n",
    "\n",
    "\n",
    "# \"datasets/doc2dial/train.tsv\"\n",
    "\n",
    "def get_features(file_path : str):\n",
    "    features = []\n",
    "    with codecs.open(file_path, \"r\", \"utf\") as f: \n",
    "        examples = [] \n",
    "        for line in f: \n",
    "            line = [s.strip() for s in line.split('\\t') if s.strip()]\n",
    "            role, session, label = line[0], line[1], line[2]\n",
    "\n",
    "            examples.append((role, session, label))\n",
    "\n",
    "        for example in examples: \n",
    "            samples = example[1].split(\"|\")\n",
    "            roles = [int(r) for r in example[0].split(\"|\")] \\\n",
    "                    if example[0].find(\"#\") != -1 \\\n",
    "                    else [int(r) for r in example[0]]\n",
    "        \n",
    "        \n",
    "            sample_input_ids = []\n",
    "            sample_segment_ids = []\n",
    "            sample_role_ids = []\n",
    "            sample_input_mask = []\n",
    "            sample_turn_ids = []\n",
    "            sample_position_ids = []\n",
    "\n",
    "            for t, s in enumerate(samples):\n",
    "                text_tokens = []\n",
    "                text_turn_ids = []\n",
    "                text_role_ids = []\n",
    "\n",
    "                texts = s.split(\"#\")\n",
    "\n",
    "                # bert-token:     [cls]  token   [sep]  token\n",
    "                # roberta-token:   <s>   token   </s>   </s> token\n",
    "                text_tokens.append(start_token)\n",
    "                text_turn_ids.append(0)\n",
    "                text_role_ids.append(roles[0])\n",
    "\n",
    "                for i, text in enumerate(texts): \n",
    "\n",
    "                    tokenized = tokenizer.tokenize(text)\n",
    "                    text_tokens.extend(tokenized)\n",
    "                    text_turn_ids.extend([i] * len(tokenized))\n",
    "                    text_role_ids.extend([roles[i]] * len(tokenized))\n",
    "\n",
    "                    if i != (len(text) - 1): \n",
    "                        text_tokens.append(sep_token)\n",
    "                        text_turn_ids.append(i)\n",
    "                        text_role_ids.append(roles[i])\n",
    "\n",
    "                max_seq_length = config.max_position_embeddings\n",
    "\n",
    "                text_tokens = text_tokens[:max_seq_length]\n",
    "                text_turn_ids = text_turn_ids[:max_seq_length]\n",
    "                text_role_ids = text_role_ids[:max_seq_length]\n",
    "\n",
    "                text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "\n",
    "\n",
    "                text_input_ids += [pad_token_id] * (max_seq_length - len(text_tokens))\n",
    "                text_input_mask = [1] * len(text_tokens) + [0] * (max_seq_length - len(text_tokens))\n",
    "                text_segment_ids = [0] * max_seq_length\n",
    "                text_position_ids = list(range(len(text_tokens))) + [0] * (max_seq_length - len(text_tokens))\n",
    "                text_turn_ids += [0] * (max_seq_length - len(text_tokens))\n",
    "                text_role_ids += [0] * (max_seq_length - len(text_tokens))\n",
    "\n",
    "\n",
    "                assert len(text_input_ids) == max_seq_length\n",
    "                assert len(text_input_mask) == max_seq_length\n",
    "                assert len(text_segment_ids) == max_seq_length\n",
    "                assert len(text_position_ids) == max_seq_length\n",
    "                assert len(text_turn_ids) == max_seq_length\n",
    "                assert len(text_role_ids) == max_seq_length\n",
    "\n",
    "                sample_input_ids.append(text_input_ids)\n",
    "                sample_turn_ids.append(text_turn_ids)\n",
    "                sample_role_ids.append(text_role_ids)\n",
    "                sample_segment_ids.append(text_segment_ids)\n",
    "                sample_position_ids.append(text_position_ids)\n",
    "                sample_input_mask.append(text_input_mask) \n",
    "\n",
    "\n",
    "            n_neg = 9\n",
    "            label_id = [1] + [0] * n_neg\n",
    "            bert_feature = BertFeatures(input_ids=sample_input_ids,\n",
    "                                        input_mask=sample_input_mask,\n",
    "                                        segment_ids=sample_segment_ids,\n",
    "                                        role_ids=sample_role_ids,\n",
    "                                        turn_ids=sample_turn_ids,\n",
    "                                        position_ids=sample_position_ids,\n",
    "                                        label_id=label_id,\n",
    "                                        guid=[None] * (1 + n_neg))\n",
    "\n",
    "            features.append(bert_feature)\n",
    "    return features\n",
    "\n",
    "features = get_features(\"datasets/doc2dial/train.tsv\")\n",
    "clustering_test = get_features(\"datasets/doc2dial/clustering_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\n",
    "import torch \n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_role_ids = torch.tensor([f.role_ids for f in features], dtype=torch.long)\n",
    "all_turn_ids = torch.tensor([f.turn_ids for f in features], dtype=torch.long)\n",
    "all_position_ids = torch.tensor([f.position_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids,\n",
    "                            all_input_mask,\n",
    "                            all_segment_ids,\n",
    "                            all_role_ids,\n",
    "                            all_turn_ids,\n",
    "                            all_position_ids,\n",
    "                            all_label_ids)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data,\n",
    "                        sampler=train_sampler,\n",
    "                        batch_size=3)\n",
    "\n",
    "\n",
    "clustering_all_input_ids = torch.tensor([f.input_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_input_mask = torch.tensor([f.input_mask for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_segment_ids = torch.tensor([f.segment_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_role_ids = torch.tensor([f.role_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_turn_ids = torch.tensor([f.turn_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_position_ids = torch.tensor([f.position_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_label_ids = torch.tensor([f.label_id for f in clustering_test], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(clustering_all_input_ids,\n",
    "                            clustering_all_input_mask,\n",
    "                            clustering_all_segment_ids,\n",
    "                            clustering_all_role_ids,\n",
    "                            clustering_all_turn_ids,\n",
    "                            clustering_all_position_ids,\n",
    "                            clustering_all_label_ids)\n",
    "\n",
    "torch.save(train_data, \"training_cache.pt\")\n",
    "torch.save(test_data, \"testing_cache.pt\")\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler = test_sampler, batch_size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Session here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_based_evaluation_at_once(features, labels, gpu_features=None, n_average=1, tsne_visualization_output=None, tasks=None, dtype='float64', logger=None, note=''):\n",
    "    \"\"\"\n",
    "    Evaluate all metrics with features\n",
    "    :param features:                        numpy.array\n",
    "    :param labels:                          list\n",
    "    :param n_average:\n",
    "    :param tsne_visualization_output:\n",
    "    :param tasks:\n",
    "    :param dtype:\n",
    "    :param logger:\n",
    "    :param note:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels = np.array(labels).astype(int)\n",
    "    if gpu_features is not None:\n",
    "        gpu_labels = torch.tensor(labels, device=gpu_features.device)\n",
    "    features = np.array(features).astype(dtype) if features is not None else None\n",
    "\n",
    "    # n_classes\n",
    "    label_set = set()\n",
    "    for s in labels:\n",
    "        label_set.add(s)\n",
    "\n",
    "    # initialize\n",
    "    RI, NMI, acc, purity = 0., 0., 0., 0.\n",
    "    clustering_time, RI_time, NMI_time, acc_time, purity_time = 0., 0., 0., 0., 0.\n",
    "    SR, SR_time = 0., 0.\n",
    "    MRR, MAP, mrr_time, map_time, ranking_time, scoring_time = 0., 0., 0., 0., 0., 0.\n",
    "    alignment, adjusted_alignment, uniformity = 0., 0., 0.\n",
    "    align_uniform_time = 0.\n",
    "\n",
    "    # KMeans\n",
    "    if 'clustering' in tasks:\n",
    "        # logger.info('KMeans Evaluation for %s tries.' % n_average)\n",
    "        for _ in range(n_average):\n",
    "            # clustering\n",
    "            pre = time()\n",
    "            clf = KMeans(n_clusters=len(label_set), max_iter=500, tol=1e-5)\n",
    "            clf.fit(features)\n",
    "            y_pred = clf.predict(features)\n",
    "            clustering_time += (time() - pre) / n_average\n",
    "\n",
    "            ## RI\n",
    "            pre = time()\n",
    "            RI += adjusted_rand_score(labels, y_pred) / n_average\n",
    "            RI_time += (time() - pre) / n_average\n",
    "\n",
    "            ## NMI\n",
    "            pre = time()\n",
    "            NMI += normalized_mutual_info_score(labels, y_pred) / n_average\n",
    "            NMI_time += (time() - pre) / n_average\n",
    "\n",
    "            ## acc\n",
    "            pre = time()\n",
    "            acc += get_accuracy(labels, y_pred) / n_average\n",
    "            acc_time += (time() - pre) / n_average\n",
    "\n",
    "            ## purity\n",
    "            pre = time()\n",
    "            purity += purity_score(labels, y_pred) / n_average\n",
    "            purity_time += (time() - pre) / n_average\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.info(\"\\nclustering_task [%s]: RI: %s NMI: %s Acc: %s Purity: %s\" % (note, RI, NMI, acc, purity))\n",
    "        logger.info(\"\\nSemantic Relatedness [%s]: SR: %s\" % (note, SR))\n",
    "        logger.info(\"\\nSession Retrieval [%s]: MRR: %s MAP: %s\" % (note, MRR, MAP))\n",
    "        logger.info(\"\\nRepresentation_Evaluation [%s]: Alignment: %.6f Alignment (adjusted): %.6f Uniformity: %.6f\" % (note, alignment, adjusted_alignment, uniformity))\n",
    "\n",
    "        tb = PrettyTable()\n",
    "        tb.field_names = ['', 'RI', 'NMI', 'Acc', 'Purity', 'SR', 'MRR', 'MAP', 'Alignment', 'Adjusted Alignment', 'Uniformity']\n",
    "        tb.add_row(['Metrics'] + ['%.2f' % (v * 100) for v in [RI, NMI, acc, purity, SR, MRR, MAP]] + ['%.2f' % v for v in [alignment, adjusted_alignment, uniformity]])\n",
    "        tb.add_row(['Times'] + ['%.2f s' % v for v in [clustering_time/4 + RI_time,\n",
    "                                                       clustering_time/4 + NMI_time,\n",
    "                                                       clustering_time/4 + acc_time,\n",
    "                                                       clustering_time/4 + purity_time,\n",
    "                                                       scoring_time/3 + SR_time,\n",
    "                                                       scoring_time/3 + ranking_time/2 + mrr_time,\n",
    "                                                       scoring_time/3 + ranking_time/2 + map_time,\n",
    "                                                       align_uniform_time/3,\n",
    "                                                       align_uniform_time/3,\n",
    "                                                       align_uniform_time/3]])\n",
    "        logger.info('\\n' + tb.__str__())\n",
    "\n",
    "    return EvaluationResult(\n",
    "        RI=RI,\n",
    "        NMI=NMI,\n",
    "        acc=acc,\n",
    "        purity=purity,\n",
    "        SR=SR,\n",
    "        MRR=MRR,\n",
    "        MAP=MAP,\n",
    "        alignment=alignment,\n",
    "        adjusted_alignment=adjusted_alignment,\n",
    "        uniformity=uniformity\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval(model: DialogueTransformer):\n",
    "    model.eval()\n",
    "    features = [] \n",
    "    with torch.no_grad(): \n",
    "        for step, batch in enumerate(test_loader):\n",
    "            batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "            output_dict = model(batch)\n",
    "\n",
    "            role2feat = {'all': 'final_feature', 'p1': 'q_feature', 'p2': 'r_feature'}\n",
    "            feature = output_dict[role2feat['all']]\n",
    "            features.append(feature)\n",
    "        features = torch.cat(features)\n",
    "\n",
    "\n",
    "    test_path = os.path.join(\"datasets/doc2dial/clustering_test.tsv\")\n",
    "    with codecs.open(test_path, \"r\", \"utf-8\") as f:\n",
    "        labels = [int(line.strip('\\n').split(\"\\t\")[-1]) for line in f]\n",
    "    evaluation_result = EvaluationResult()\n",
    "\n",
    "    n_average = max(3, 10 - features.shape[0] // 500)\n",
    "    er = feature_based_evaluation_at_once(features=features.cpu(),\n",
    "                                            labels=labels,\n",
    "                                            n_average=n_average,\n",
    "                                            tsne_visualization_output=None,\n",
    "                                            tasks=['clustering'],\n",
    "                                            dtype='float32',\n",
    "                                            logger=None,\n",
    "                                            note=','.join(['test', 'mean', 'all']))\n",
    "    evaluation_result.RI = er.RI\n",
    "    evaluation_result.NMI = er.NMI\n",
    "    evaluation_result.acc = er.acc\n",
    "    evaluation_result.purity = er.purity\n",
    "\n",
    "    return evaluation_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization import BERTAdam\n",
    "\n",
    "\n",
    "d2vmodel = DialogueTransformer().to(torch.device(\"mps\"))\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}]\n",
    "optimizer = BERTAdam(optimizer_grouped_parameters, lr=1e-5, warmup=0.1, t_total=line_statistics(\"datasets/doc2dial/train.tsv\"))\n",
    "\n",
    "res = EvaluationResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel.train()\n",
    "for step, batch in enumerate(train_loader):\n",
    "    batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "    output_dict = d2vmodel(batch, strategy='mean_by_role')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 60.29 GB, other allocations: 684.78 MB, max allowed: 61.20 GB). Tried to allocate 604.70 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m      5\u001b[39m     batch = \u001b[38;5;28mtuple\u001b[39m(t.to(torch.device(\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output_dict = \u001b[43md2vmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean_by_role\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     loss = output_dict[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m     loss = loss / \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mDialogueTransformer.forward\u001b[39m\u001b[34m(self, data, strategy, output_attention)\u001b[39m\n\u001b[32m     36\u001b[39m a_attention_mask = (attention_mask * role_a_mask)\n\u001b[32m     37\u001b[39m b_attention_mask = (attention_mask * role_b_mask)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m self_output, pooled_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mturn_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m q_self_output = self_output * a_attention_mask.unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     42\u001b[39m r_self_output = self_output * b_attention_mask.unsqueeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mDialogueTransformer.encoder\u001b[39m\u001b[34m(self, *x)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, *x):\n\u001b[32m    104\u001b[39m     input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids = x    \n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     all_output = output[\u001b[33m'\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    113\u001b[39m     pooler_output = output[\u001b[33m'\u001b[39m\u001b[33mpooler_output\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:869\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    862\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    866\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    867\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    882\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:618\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    607\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    608\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    609\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m         output_attentions,\n\u001b[32m    616\u001b[39m     )\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:507\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    496\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    497\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    506\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    516\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:434\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    425\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    426\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    433\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    444\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dial2vec/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:357\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    353\u001b[39m is_causal = (\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    355\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    367\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 60.29 GB, other allocations: 684.78 MB, max allowed: 61.20 GB). Tried to allocate 604.70 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "steps = 0 \n",
    "d2vmodel.train()\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "        output_dict = d2vmodel(batch, strategy='mean_by_role')\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "        loss = loss / 1.0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if steps % 20 == 0: \n",
    "            print(\"Loss: \", loss.item())\n",
    "        if steps % 100 == 0:\n",
    "            new_res = eval(d2vmodel)\n",
    "            print(new_res.acc, res.acc)\n",
    "            if new_res > res:\n",
    "                print(\"new best: \", new_res.acc)\n",
    "                torch.save(d2vmodel.state_dict(), \"dial2vec_model.bin\")\n",
    "                res = new_res\n",
    "\n",
    "\n",
    "            d2vmodel.train()\n",
    "\n",
    "        steps += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dial2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
