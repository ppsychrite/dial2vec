{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/croarkin.kyle/Desktop/dial2vec/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import logging \n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = \"mps\") \n",
    "tokenizer = model.tokenizer\n",
    "from metrics import * \n",
    "start_token, sep_token, pad_token_id = tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token_id\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import os \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_statistics(file_name):\n",
    "    \"\"\"\n",
    "    统计文件行数\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        return 0\n",
    "\n",
    "    return 6948 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "\n",
    "class PoolingAverage(nn.Module):\n",
    "    def __init__(self, eps=1e-12):\n",
    "        super(PoolingAverage, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = lambda x, m: x * torch.unsqueeze(m, dim=-1)\n",
    "        reduce_mean = lambda x, m: torch.sum(mul_mask(x, m), dim=1) / (torch.sum(m, dim=1, keepdims=True) + self.eps)\n",
    "\n",
    "        avg_output = reduce_mean(hidden_states, attention_mask)\n",
    "        return avg_output\n",
    "\n",
    "    def equal_forward(self, hidden_states, attention_mask):\n",
    "        mul_mask = hidden_states * attention_mask.unsqueeze(-1)\n",
    "        avg_output = torch.sum(mul_mask, dim=1) / (torch.sum(attention_mask, dim=1, keepdim=True) + self.eps)\n",
    "        return avg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DialogueTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DialogueTransformer, self).__init__()\n",
    "        self.bert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = \"mps\") \n",
    "\n",
    "        hf_model = model._first_module().auto_model  # or model[0].auto_model\n",
    "        self.config = hf_model.config\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.labels_data = None\n",
    "        self.sample_nums = 10\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.avg = PoolingAverage(eps=1e-6)\n",
    "        self.logger = logger  \n",
    "\n",
    "    def forward(self, data, strategy='mean_by_role', output_attention=False):\n",
    "\n",
    "\n",
    "        if len(data) == 7:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels = data\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids, role_ids, turn_ids, position_ids, labels, guids = data\n",
    "\n",
    "        input_ids = input_ids.view(input_ids.size()[0] * input_ids.size()[1], input_ids.size()[-1])\n",
    "        attention_mask = attention_mask.view(attention_mask.size()[0] * attention_mask.size()[1], attention_mask.size()[-1])\n",
    "        token_type_ids = token_type_ids.view(token_type_ids.size()[0] * token_type_ids.size()[1], token_type_ids.size()[-1])\n",
    "        role_ids = role_ids.view(role_ids.size()[0] * role_ids.size()[1], role_ids.size()[-1])\n",
    "        turn_ids = turn_ids.view(turn_ids.size()[0] * turn_ids.size()[1], turn_ids.size()[-1])\n",
    "        position_ids = position_ids.view(position_ids.size()[0] * position_ids.size()[1], position_ids.size()[-1])\n",
    "\n",
    "        one_mask = torch.ones_like(role_ids)\n",
    "        zero_mask = torch.zeros_like(role_ids)\n",
    "        role_a_mask = torch.where(role_ids == 0, one_mask, zero_mask)\n",
    "        role_b_mask = torch.where(role_ids == 1, one_mask, zero_mask)\n",
    "\n",
    "        sep_token_id = self.bert.tokenizer.sep_token_id \n",
    "        a_attention_mask = (attention_mask * role_a_mask)\n",
    "        b_attention_mask = (attention_mask * role_b_mask)\n",
    "\n",
    "        self_output, pooled_output = self.encoder(input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids)\n",
    "\n",
    "        q_self_output = self_output * a_attention_mask.unsqueeze(-1)\n",
    "        r_self_output = self_output * b_attention_mask.unsqueeze(-1)\n",
    "\n",
    "        self_output = self_output * attention_mask.unsqueeze(-1)\n",
    "        w = torch.matmul(q_self_output, r_self_output.transpose(-1, -2))\n",
    "\n",
    "        if turn_ids is not None:\n",
    "            view_turn_mask = turn_ids.unsqueeze(1).repeat(1, self.bert.max_seq_length, 1)\n",
    "            view_turn_mask_transpose = view_turn_mask.transpose(2, 1)\n",
    "            view_range_mask = torch.where(abs(view_turn_mask_transpose - view_turn_mask) <= 1000,\n",
    "                                          torch.ones_like(view_turn_mask),\n",
    "                                          torch.zeros_like(view_turn_mask))\n",
    "            filtered_w = w * view_range_mask\n",
    "\n",
    "        q_cross_output = torch.matmul(filtered_w.permute(0, 2, 1), q_self_output)\n",
    "        r_cross_output = torch.matmul(filtered_w, r_self_output)\n",
    "\n",
    "        q_self_output = self.avg(q_self_output, a_attention_mask)\n",
    "        q_cross_output = self.avg(q_cross_output, b_attention_mask)\n",
    "        r_self_output = self.avg(r_self_output, b_attention_mask)\n",
    "        r_cross_output = self.avg(r_cross_output, a_attention_mask)\n",
    "\n",
    "        self_output = self.avg(self_output, attention_mask)\n",
    "        q_self_output = q_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        q_cross_output = q_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_self_output = r_self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        r_cross_output = r_cross_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        self_output = self_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "        pooled_output = pooled_output.view(-1, self.sample_nums, self.config.hidden_size)\n",
    "\n",
    "        output = self_output[:, 0, :]\n",
    "        q_output = q_self_output[:, 0, :]\n",
    "        r_output = r_self_output[:, 0, :]\n",
    "        q_contrastive_output = q_cross_output[:, 0, :]\n",
    "        r_contrastive_output = r_cross_output[:, 0, :]\n",
    "\n",
    "        logit_q = []\n",
    "        logit_r = []\n",
    "        for i in range(self.sample_nums):\n",
    "            cos_q = self.calc_cos(q_self_output[:, i, :], q_cross_output[:, i, :])\n",
    "            cos_r = self.calc_cos(r_self_output[:, i, :], r_cross_output[:, i, :])\n",
    "            logit_r.append(cos_r)\n",
    "            logit_q.append(cos_q)\n",
    "\n",
    "        logit_r = torch.stack(logit_r, dim=1)\n",
    "        logit_q = torch.stack(logit_q, dim=1)\n",
    "\n",
    "        loss_r = self.calc_loss(logit_r, labels)\n",
    "        loss_q = self.calc_loss(logit_q, labels)\n",
    "\n",
    "        if strategy not in ['mean', 'mean_by_role']:\n",
    "            raise ValueError('Unknown strategy: [%s]' % strategy)\n",
    "\n",
    "        output_dict = {'loss': loss_r + loss_q,\n",
    "                       'final_feature': output if strategy == 'mean' else q_output + r_output,\n",
    "                       'q_feature': q_output,\n",
    "                       'r_feature': r_output,\n",
    "                       'attention': w}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def encoder(self, *x):\n",
    "        input_ids, attention_mask, token_type_ids, position_ids, turn_ids, role_ids = x    \n",
    "\n",
    "        transformer = self.bert[0].auto_model\n",
    "        output = transformer(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        all_output = output['hidden_states']\n",
    "        pooler_output = output['pooler_output']\n",
    "        return all_output[-1], pooler_output\n",
    "\n",
    "    def calc_cos(self, x, y):\n",
    "        cos = torch.cosine_similarity(x, y, dim=1)\n",
    "        cos = cos / 1.0 # cos = cos / 2.0\n",
    "        return cos\n",
    "\n",
    "    def calc_loss(self, pred, labels):\n",
    "        loss = -torch.mean(self.log_softmax(pred) * labels)\n",
    "        return loss\n",
    "\n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "\n",
    "    def get_labels_data(self):\n",
    "        return self.labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "class BertFeatures():\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, role_ids, label_id, turn_ids=None, position_ids=None, guid=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.role_ids = role_ids\n",
    "        self.turn_ids = turn_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.label_id = label_id\n",
    "        self.guid = guid\n",
    "\n",
    "        self.batch_size = len(self.input_ids)\n",
    "import codecs  \n",
    "\n",
    "\n",
    "\n",
    "# \"datasets/doc2dial/train.tsv\"\n",
    "\n",
    "def get_features(file_path : str):\n",
    "    features = []\n",
    "    with codecs.open(file_path, \"r\", \"utf\") as f: \n",
    "        examples = [] \n",
    "        for line in f: \n",
    "            line = [s.strip() for s in line.split('\\t') if s.strip()]\n",
    "            role, session, label = line[0], line[1], line[2]\n",
    "\n",
    "            examples.append((role, session, label))\n",
    "\n",
    "        for example in examples: \n",
    "            samples = example[1].split(\"|\")\n",
    "            roles = [int(r) for r in example[0].split(\"|\")] \\\n",
    "                    if example[0].find(\"#\") != -1 \\\n",
    "                    else [int(r) for r in example[0]]\n",
    "        \n",
    "        \n",
    "            sample_input_ids = []\n",
    "            sample_segment_ids = []\n",
    "            sample_role_ids = []\n",
    "            sample_input_mask = []\n",
    "            sample_turn_ids = []\n",
    "            sample_position_ids = []\n",
    "\n",
    "            for t, s in enumerate(samples):\n",
    "                text_tokens = []\n",
    "                text_turn_ids = []\n",
    "                text_role_ids = []\n",
    "\n",
    "                texts = s.split(\"#\")\n",
    "\n",
    "                # bert-token:     [cls]  token   [sep]  token\n",
    "                # roberta-token:   <s>   token   </s>   </s> token\n",
    "\n",
    "\n",
    "                text_tokens.append(start_token)\n",
    "                text_turn_ids.append(0)\n",
    "                text_role_ids.append(roles[0])\n",
    "\n",
    "                for i, text in enumerate(texts): \n",
    "\n",
    "                    tokenized = tokenizer.tokenize(text)\n",
    "                    text_tokens.extend(tokenized)\n",
    "                    text_turn_ids.extend([i] * len(tokenized))\n",
    "                    text_role_ids.extend([roles[i]] * len(tokenized))\n",
    "\n",
    "                    if i != (len(text) - 1): \n",
    "                        text_tokens.append(sep_token)\n",
    "                        text_turn_ids.append(i)\n",
    "                        text_role_ids.append(roles[i])\n",
    "\n",
    "                text_tokens = text_tokens[:model.max_seq_length]\n",
    "                text_turn_ids = text_turn_ids[:model.max_seq_length]\n",
    "                text_role_ids = text_role_ids[:model.max_seq_length]\n",
    "\n",
    "                text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "\n",
    "\n",
    "                text_input_ids += [pad_token_id] * (model.max_seq_length - len(text_tokens))\n",
    "                text_input_mask = [1] * len(text_tokens) + [0] * (model.max_seq_length - len(text_tokens))\n",
    "                text_segment_ids = [0] * model.max_seq_length\n",
    "                text_position_ids = list(range(len(text_tokens))) + [0] * (model.max_seq_length - len(text_tokens))\n",
    "                text_turn_ids += [0] * (model.max_seq_length - len(text_tokens))\n",
    "                text_role_ids += [0] * (model.max_seq_length - len(text_tokens))\n",
    "\n",
    "\n",
    "                assert len(text_input_ids) == model.max_seq_length\n",
    "                assert len(text_input_mask) == model.max_seq_length\n",
    "                assert len(text_segment_ids) == model.max_seq_length\n",
    "                assert len(text_position_ids) == model.max_seq_length\n",
    "                assert len(text_turn_ids) == model.max_seq_length\n",
    "                assert len(text_role_ids) == model.max_seq_length\n",
    "\n",
    "                sample_input_ids.append(text_input_ids)\n",
    "                sample_turn_ids.append(text_turn_ids)\n",
    "                sample_role_ids.append(text_role_ids)\n",
    "                sample_segment_ids.append(text_segment_ids)\n",
    "                sample_position_ids.append(text_position_ids)\n",
    "                sample_input_mask.append(text_input_mask) \n",
    "\n",
    "\n",
    "            n_neg = 9\n",
    "            label_id = [1] + [0] * n_neg\n",
    "            bert_feature = BertFeatures(input_ids=sample_input_ids,\n",
    "                                        input_mask=sample_input_mask,\n",
    "                                        segment_ids=sample_segment_ids,\n",
    "                                        role_ids=sample_role_ids,\n",
    "                                        turn_ids=sample_turn_ids,\n",
    "                                        position_ids=sample_position_ids,\n",
    "                                        label_id=label_id,\n",
    "                                        guid=[None] * (1 + n_neg))\n",
    "\n",
    "            features.append(bert_feature)\n",
    "    return features\n",
    "\n",
    "features = get_features(\"datasets/doc2dial/train.tsv\")\n",
    "clustering_test = get_features(\"datasets/doc2dial/clustering_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DistributedSampler\n",
    "import torch \n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_role_ids = torch.tensor([f.role_ids for f in features], dtype=torch.long)\n",
    "all_turn_ids = torch.tensor([f.turn_ids for f in features], dtype=torch.long)\n",
    "all_position_ids = torch.tensor([f.position_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids,\n",
    "                            all_input_mask,\n",
    "                            all_segment_ids,\n",
    "                            all_role_ids,\n",
    "                            all_turn_ids,\n",
    "                            all_position_ids,\n",
    "                            all_label_ids)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data,\n",
    "                        sampler=train_sampler,\n",
    "                        batch_size=5)\n",
    "\n",
    "\n",
    "clustering_all_input_ids = torch.tensor([f.input_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_input_mask = torch.tensor([f.input_mask for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_segment_ids = torch.tensor([f.segment_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_role_ids = torch.tensor([f.role_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_turn_ids = torch.tensor([f.turn_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_position_ids = torch.tensor([f.position_ids for f in clustering_test], dtype=torch.long)\n",
    "clustering_all_label_ids = torch.tensor([f.label_id for f in clustering_test], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(clustering_all_input_ids,\n",
    "                            clustering_all_input_mask,\n",
    "                            clustering_all_segment_ids,\n",
    "                            clustering_all_role_ids,\n",
    "                            clustering_all_turn_ids,\n",
    "                            clustering_all_position_ids,\n",
    "                            clustering_all_label_ids)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler = test_sampler, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_based_evaluation_at_once(features, labels, gpu_features=None, n_average=1, tsne_visualization_output=None, tasks=None, dtype='float64', logger=None, note=''):\n",
    "    \"\"\"\n",
    "    Evaluate all metrics with features\n",
    "    :param features:                        numpy.array\n",
    "    :param labels:                          list\n",
    "    :param n_average:\n",
    "    :param tsne_visualization_output:\n",
    "    :param tasks:\n",
    "    :param dtype:\n",
    "    :param logger:\n",
    "    :param note:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels = np.array(labels).astype(int)\n",
    "    if gpu_features is not None:\n",
    "        gpu_labels = torch.tensor(labels, device=gpu_features.device)\n",
    "    features = np.array(features).astype(dtype) if features is not None else None\n",
    "\n",
    "    # n_classes\n",
    "    label_set = set()\n",
    "    for s in labels:\n",
    "        label_set.add(s)\n",
    "\n",
    "    # initialize\n",
    "    RI, NMI, acc, purity = 0., 0., 0., 0.\n",
    "    clustering_time, RI_time, NMI_time, acc_time, purity_time = 0., 0., 0., 0., 0.\n",
    "    SR, SR_time = 0., 0.\n",
    "    MRR, MAP, mrr_time, map_time, ranking_time, scoring_time = 0., 0., 0., 0., 0., 0.\n",
    "    alignment, adjusted_alignment, uniformity = 0., 0., 0.\n",
    "    align_uniform_time = 0.\n",
    "\n",
    "    # KMeans\n",
    "    if 'clustering' in tasks:\n",
    "        # logger.info('KMeans Evaluation for %s tries.' % n_average)\n",
    "        for _ in range(n_average):\n",
    "            # clustering\n",
    "            pre = time()\n",
    "            clf = KMeans(n_clusters=len(label_set), max_iter=500, tol=1e-5)\n",
    "            clf.fit(features)\n",
    "            y_pred = clf.predict(features)\n",
    "            clustering_time += (time() - pre) / n_average\n",
    "\n",
    "            ## RI\n",
    "            pre = time()\n",
    "            RI += adjusted_rand_score(labels, y_pred) / n_average\n",
    "            RI_time += (time() - pre) / n_average\n",
    "\n",
    "            ## NMI\n",
    "            pre = time()\n",
    "            NMI += normalized_mutual_info_score(labels, y_pred) / n_average\n",
    "            NMI_time += (time() - pre) / n_average\n",
    "\n",
    "            ## acc\n",
    "            pre = time()\n",
    "            acc += get_accuracy(labels, y_pred) / n_average\n",
    "            acc_time += (time() - pre) / n_average\n",
    "\n",
    "            ## purity\n",
    "            pre = time()\n",
    "            purity += purity_score(labels, y_pred) / n_average\n",
    "            purity_time += (time() - pre) / n_average\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.info(\"\\nclustering_task [%s]: RI: %s NMI: %s Acc: %s Purity: %s\" % (note, RI, NMI, acc, purity))\n",
    "        logger.info(\"\\nSemantic Relatedness [%s]: SR: %s\" % (note, SR))\n",
    "        logger.info(\"\\nSession Retrieval [%s]: MRR: %s MAP: %s\" % (note, MRR, MAP))\n",
    "        logger.info(\"\\nRepresentation_Evaluation [%s]: Alignment: %.6f Alignment (adjusted): %.6f Uniformity: %.6f\" % (note, alignment, adjusted_alignment, uniformity))\n",
    "\n",
    "        tb = PrettyTable()\n",
    "        tb.field_names = ['', 'RI', 'NMI', 'Acc', 'Purity', 'SR', 'MRR', 'MAP', 'Alignment', 'Adjusted Alignment', 'Uniformity']\n",
    "        tb.add_row(['Metrics'] + ['%.2f' % (v * 100) for v in [RI, NMI, acc, purity, SR, MRR, MAP]] + ['%.2f' % v for v in [alignment, adjusted_alignment, uniformity]])\n",
    "        tb.add_row(['Times'] + ['%.2f s' % v for v in [clustering_time/4 + RI_time,\n",
    "                                                       clustering_time/4 + NMI_time,\n",
    "                                                       clustering_time/4 + acc_time,\n",
    "                                                       clustering_time/4 + purity_time,\n",
    "                                                       scoring_time/3 + SR_time,\n",
    "                                                       scoring_time/3 + ranking_time/2 + mrr_time,\n",
    "                                                       scoring_time/3 + ranking_time/2 + map_time,\n",
    "                                                       align_uniform_time/3,\n",
    "                                                       align_uniform_time/3,\n",
    "                                                       align_uniform_time/3]])\n",
    "        logger.info('\\n' + tb.__str__())\n",
    "\n",
    "    return EvaluationResult(\n",
    "        RI=RI,\n",
    "        NMI=NMI,\n",
    "        acc=acc,\n",
    "        purity=purity,\n",
    "        SR=SR,\n",
    "        MRR=MRR,\n",
    "        MAP=MAP,\n",
    "        alignment=alignment,\n",
    "        adjusted_alignment=adjusted_alignment,\n",
    "        uniformity=uniformity\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval(model: DialogueTransformer):\n",
    "    model.eval()\n",
    "    features = [] \n",
    "    with torch.no_grad(): \n",
    "        for step, batch in enumerate(test_loader):\n",
    "            batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "            output_dict = model(batch)\n",
    "\n",
    "            role2feat = {'all': 'final_feature', 'p1': 'q_feature', 'p2': 'r_feature'}\n",
    "            feature = output_dict[role2feat['all']]\n",
    "            features.append(feature)\n",
    "        features = torch.cat(features)\n",
    "\n",
    "\n",
    "    test_path = os.path.join(\"datasets/doc2dial/clustering_test.tsv\")\n",
    "    with codecs.open(test_path, \"r\", \"utf-8\") as f:\n",
    "        labels = [int(line.strip('\\n').split(\"\\t\")[-1]) for line in f]\n",
    "    evaluation_result = EvaluationResult()\n",
    "\n",
    "    n_average = max(3, 10 - features.shape[0] // 500)\n",
    "    er = feature_based_evaluation_at_once(features=features.cpu(),\n",
    "                                            labels=labels,\n",
    "                                            n_average=n_average,\n",
    "                                            tsne_visualization_output=None,\n",
    "                                            tasks=['clustering'],\n",
    "                                            dtype='float32',\n",
    "                                            logger=None,\n",
    "                                            note=','.join(['test', 'mean', 'all']))\n",
    "    evaluation_result.RI = er.RI\n",
    "    evaluation_result.NMI = er.NMI\n",
    "    evaluation_result.acc = er.acc\n",
    "    evaluation_result.purity = er.purity\n",
    "\n",
    "    return evaluation_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/26/2025 12:54:34 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.4601125717163086\n",
      "0.8730879139351151 0.0\n",
      "new best:  0.8730879139351151\n",
      "Loss:  0.4602244198322296\n",
      "Loss:  0.45927125215530396\n",
      "Loss:  0.45874667167663574\n",
      "Loss:  0.4507601857185364\n",
      "Loss:  0.4451795518398285\n",
      "0.7325600941334678 0.8730879139351151\n",
      "Loss:  0.4314385652542114\n",
      "Loss:  0.3934085965156555\n",
      "Loss:  0.3833683729171753\n",
      "Loss:  0.3733525276184082\n",
      "Loss:  0.3497527837753296\n",
      "0.7453353504790721 0.8730879139351151\n",
      "Loss:  0.3690670132637024\n",
      "Loss:  0.29732853174209595\n",
      "Loss:  0.3241916596889496\n",
      "Loss:  0.334039568901062\n",
      "Loss:  0.31926727294921875\n",
      "0.760632038998151 0.8730879139351151\n",
      "Loss:  0.3249553442001343\n",
      "Loss:  0.2938807010650635\n",
      "Loss:  0.31351521611213684\n",
      "Loss:  0.28149744868278503\n",
      "Loss:  0.2919015884399414\n",
      "0.6844847873592201 0.8730879139351151\n",
      "Loss:  0.28605756163597107\n",
      "Loss:  0.3093002140522003\n",
      "Loss:  0.26872217655181885\n",
      "Loss:  0.2809312641620636\n",
      "Loss:  0.25306737422943115\n",
      "0.7132291141368298 0.8730879139351151\n",
      "Loss:  0.283758282661438\n",
      "Loss:  0.2690661549568176\n",
      "Loss:  0.2630433738231659\n",
      "Loss:  0.20200979709625244\n",
      "Loss:  0.29441362619400024\n",
      "0.7809715918641787 0.8730879139351151\n",
      "Loss:  0.27120769023895264\n",
      "Loss:  0.22666585445404053\n",
      "Loss:  0.19896632432937622\n",
      "Loss:  0.27844980359077454\n",
      "Loss:  0.21950995922088623\n",
      "0.7411329635232812 0.8730879139351151\n",
      "Loss:  0.22181099653244019\n",
      "Loss:  0.22366642951965332\n",
      "Loss:  0.1911316215991974\n",
      "Loss:  0.22639282047748566\n",
      "Loss:  0.20815736055374146\n",
      "0.7058329130946378 0.8730879139351151\n",
      "Loss:  0.2732718288898468\n",
      "Loss:  0.22669094800949097\n",
      "Loss:  0.21646235883235931\n",
      "Loss:  0.2769322991371155\n",
      "Loss:  0.1943943053483963\n",
      "0.7621448983022356 0.8730879139351151\n",
      "Loss:  0.3323305547237396\n",
      "Loss:  0.25593534111976624\n",
      "Loss:  0.19622719287872314\n",
      "Loss:  0.2833958864212036\n",
      "Loss:  0.24394360184669495\n",
      "0.6466633047571021 0.8730879139351151\n",
      "Loss:  0.3034598231315613\n",
      "Loss:  0.21371760964393616\n",
      "Loss:  0.2094642072916031\n",
      "Loss:  0.23871789872646332\n",
      "Loss:  0.2462892085313797\n",
      "0.6817952597075139 0.8730879139351151\n",
      "Loss:  0.28267359733581543\n",
      "Loss:  0.1781591922044754\n",
      "Loss:  0.194918692111969\n",
      "Loss:  0.25931060314178467\n",
      "Loss:  0.21232080459594727\n",
      "0.7350815263069422 0.8730879139351151\n",
      "Loss:  0.17879122495651245\n",
      "Loss:  0.2135854959487915\n",
      "Loss:  0.19956360757350922\n",
      "Loss:  0.1923084408044815\n",
      "Loss:  0.27230656147003174\n",
      "0.7910573205580771 0.8730879139351151\n",
      "Loss:  0.2457903027534485\n",
      "Loss:  0.19362232089042664\n",
      "Loss:  0.2016192078590393\n",
      "Loss:  0.2908642888069153\n",
      "Loss:  0.2457505762577057\n",
      "0.7270129433518239 0.8730879139351151\n",
      "Loss:  0.17328375577926636\n",
      "Loss:  0.22938048839569092\n",
      "Loss:  0.21716256439685822\n",
      "Loss:  0.1896963268518448\n",
      "Loss:  0.1799372434616089\n",
      "0.6710371491006892 0.8730879139351151\n",
      "Loss:  0.18717360496520996\n",
      "Loss:  0.20567993819713593\n",
      "Loss:  0.18453076481819153\n",
      "Loss:  0.17059481143951416\n",
      "Loss:  0.24741896986961365\n",
      "0.6902000336190957 0.8730879139351151\n",
      "Loss:  0.21294327080249786\n",
      "Loss:  0.2465744912624359\n",
      "Loss:  0.20812273025512695\n",
      "Loss:  0.1857111155986786\n",
      "Loss:  0.22744262218475342\n",
      "0.7286938981341403 0.8730879139351151\n",
      "Loss:  0.2288372814655304\n",
      "Loss:  0.22092841565608978\n",
      "Loss:  0.18338404595851898\n",
      "Loss:  0.18899090588092804\n",
      "Loss:  0.2420552372932434\n",
      "0.7236510337871911 0.8730879139351151\n",
      "Loss:  0.22451096773147583\n",
      "Loss:  0.24454081058502197\n",
      "Loss:  0.1810067892074585\n",
      "Loss:  0.24008604884147644\n",
      "Loss:  0.23309779167175293\n",
      "0.7394520087409648 0.8730879139351151\n",
      "Loss:  0.24973899126052856\n",
      "Loss:  0.22274616360664368\n",
      "Loss:  0.2395516335964203\n",
      "Loss:  0.17648297548294067\n",
      "Loss:  0.18508993089199066\n",
      "0.7517229786518742 0.8730879139351151\n",
      "Loss:  0.17911550402641296\n",
      "Loss:  0.16807880997657776\n",
      "Loss:  0.1891499161720276\n",
      "Loss:  0.18008601665496826\n",
      "Loss:  0.19392454624176025\n",
      "0.8130778282064212 0.8730879139351151\n",
      "Loss:  0.2267005741596222\n",
      "Loss:  0.21178603172302246\n",
      "Loss:  0.17371264100074768\n",
      "Loss:  0.20880502462387085\n",
      "Loss:  0.16791394352912903\n",
      "0.71121196839805 0.8730879139351151\n",
      "Loss:  0.18678542971611023\n",
      "Loss:  0.21235844492912292\n",
      "Loss:  0.18679167330265045\n",
      "Loss:  0.2174597978591919\n",
      "Loss:  0.21192598342895508\n",
      "0.6779290637081863 0.8730879139351151\n",
      "Loss:  0.18106891214847565\n",
      "Loss:  0.20350559055805206\n",
      "Loss:  0.23175349831581116\n",
      "Loss:  0.23258572816848755\n",
      "Loss:  0.21966883540153503\n",
      "0.7012943351823836 0.8730879139351151\n",
      "Loss:  0.21531149744987488\n",
      "Loss:  0.18179726600646973\n",
      "Loss:  0.19206121563911438\n",
      "Loss:  0.1837857961654663\n",
      "Loss:  0.17468446493148804\n",
      "0.7014624306606152 0.8730879139351151\n",
      "Loss:  0.1736905574798584\n",
      "Loss:  0.17792043089866638\n",
      "Loss:  0.20096132159233093\n",
      "Loss:  0.18356463313102722\n",
      "Loss:  0.17461590468883514\n",
      "0.8231635569003194 0.8730879139351151\n",
      "Loss:  0.2491896003484726\n",
      "Loss:  0.23092815279960632\n",
      "Loss:  0.17484921216964722\n",
      "Loss:  0.23835571110248566\n",
      "Loss:  0.1667763590812683\n",
      "0.7784501596907043 0.8730879139351151\n",
      "Loss:  0.16401463747024536\n",
      "Loss:  0.21041753888130188\n",
      "Loss:  0.1801871359348297\n",
      "Loss:  0.19426529109477997\n",
      "Loss:  0.22985883057117462\n",
      "0.7520591696083376 0.8730879139351151\n",
      "Loss:  0.2674240469932556\n",
      "Loss:  0.17345088720321655\n",
      "Loss:  0.2336711883544922\n",
      "Loss:  0.16528648138046265\n",
      "Loss:  0.17776857316493988\n",
      "0.7246596066565809 0.8730879139351151\n",
      "Loss:  0.17226547002792358\n",
      "Loss:  0.16536203026771545\n",
      "Loss:  0.18745175004005432\n",
      "Loss:  0.1900135576725006\n",
      "Loss:  0.17836594581604004\n",
      "0.7209615061354849 0.8730879139351151\n",
      "Loss:  0.17727312445640564\n",
      "Loss:  0.16826389729976654\n",
      "Loss:  0.2237393856048584\n",
      "Loss:  0.16524192690849304\n",
      "Loss:  0.16214977204799652\n",
      "0.7453353504790721 0.8730879139351151\n",
      "Loss:  0.16676460206508636\n",
      "Loss:  0.1729251593351364\n",
      "Loss:  0.18121978640556335\n",
      "Loss:  0.1926964819431305\n",
      "Loss:  0.1839020550251007\n",
      "0.7545806017818121 0.8730879139351151\n",
      "Loss:  0.22843825817108154\n",
      "Loss:  0.17962493002414703\n",
      "Loss:  0.23344188928604126\n",
      "Loss:  0.16296085715293884\n",
      "Loss:  0.19998523592948914\n",
      "0.8155992603798958 0.8730879139351151\n",
      "Loss:  0.2860443592071533\n",
      "Loss:  0.2198089063167572\n",
      "Loss:  0.2066137194633484\n",
      "Loss:  0.18407240509986877\n",
      "Loss:  0.1787320375442505\n",
      "0.7853420742982014 0.8730879139351151\n",
      "Loss:  0.20268571376800537\n",
      "Loss:  0.18204671144485474\n",
      "Loss:  0.16310665011405945\n",
      "Loss:  0.2359876185655594\n",
      "Loss:  0.16443324089050293\n",
      "0.7838292149941167 0.8730879139351151\n",
      "Loss:  0.25082677602767944\n",
      "Loss:  0.16652335226535797\n",
      "Loss:  0.22298625111579895\n",
      "Loss:  0.20933961868286133\n",
      "Loss:  0.1697896420955658\n",
      "0.7396201042191966 0.8730879139351151\n",
      "Loss:  0.17542573809623718\n",
      "Loss:  0.17835783958435059\n",
      "Loss:  0.17695799469947815\n",
      "Loss:  0.1760016679763794\n",
      "Loss:  0.1633477807044983\n",
      "0.755084888216507 0.8730879139351151\n",
      "Loss:  0.2161361575126648\n",
      "Loss:  0.20328767597675323\n",
      "Loss:  0.17413504421710968\n",
      "Loss:  0.17229582369327545\n",
      "Loss:  0.20357656478881836\n",
      "0.7766011094301564 0.8730879139351151\n",
      "Loss:  0.16494077444076538\n",
      "Loss:  0.20066064596176147\n",
      "Loss:  0.21024860441684723\n",
      "Loss:  0.2971690893173218\n",
      "Loss:  0.1695239245891571\n",
      "0.7424777273491343 0.8730879139351151\n",
      "Loss:  0.1714610904455185\n",
      "Loss:  0.1757984459400177\n",
      "Loss:  0.16620343923568726\n",
      "Loss:  0.1648038625717163\n",
      "Loss:  0.168668732047081\n",
      "0.7340729534375525 0.8730879139351151\n",
      "Loss:  0.18753167986869812\n",
      "Loss:  0.17939388751983643\n",
      "Loss:  0.18996328115463257\n",
      "Loss:  0.16440965235233307\n",
      "Loss:  0.1760748028755188\n",
      "0.8066902000336191 0.8730879139351151\n",
      "Loss:  0.18283510208129883\n",
      "Loss:  0.20567360520362854\n",
      "Loss:  0.2066822350025177\n",
      "Loss:  0.22400376200675964\n",
      "Loss:  0.18250493705272675\n",
      "0.7912254160363086 0.8730879139351151\n",
      "Loss:  0.22461453080177307\n",
      "Loss:  0.1738869547843933\n",
      "Loss:  0.17333662509918213\n",
      "Loss:  0.19055390357971191\n",
      "Loss:  0.22950124740600586\n",
      "0.8056816271642292 0.8730879139351151\n",
      "Loss:  0.19119298458099365\n",
      "Loss:  0.19274112582206726\n",
      "Loss:  0.16481058299541473\n",
      "Loss:  0.18030183017253876\n",
      "Loss:  0.22429776191711426\n",
      "0.773575390821987 0.8730879139351151\n",
      "Loss:  0.16736988723278046\n",
      "Loss:  0.16769059002399445\n",
      "Loss:  0.16366717219352722\n",
      "Loss:  0.1901470422744751\n",
      "Loss:  0.1720162332057953\n",
      "0.7468482097831568 0.8730879139351151\n",
      "Loss:  0.17076557874679565\n",
      "Loss:  0.1708526462316513\n",
      "Loss:  0.1902274489402771\n",
      "Loss:  0.1745784878730774\n",
      "Loss:  0.18578201532363892\n",
      "0.741301059001513 0.8730879139351151\n",
      "Loss:  0.17312467098236084\n",
      "Loss:  0.19513511657714844\n",
      "Loss:  0.16701765358448029\n",
      "Loss:  0.16585180163383484\n",
      "Loss:  0.17226891219615936\n",
      "0.7838292149941166 0.8730879139351151\n",
      "Loss:  0.16345791518688202\n",
      "Loss:  0.1679133176803589\n",
      "Loss:  0.21046298742294312\n",
      "Loss:  0.163443922996521\n",
      "Loss:  0.17367568612098694\n",
      "0.767523953605648 0.8730879139351151\n",
      "Loss:  0.17357096076011658\n",
      "Loss:  0.2716289460659027\n",
      "Loss:  0.17492032051086426\n",
      "Loss:  0.1773015558719635\n",
      "Loss:  0.19604405760765076\n",
      "0.782316355690032 0.8730879139351151\n",
      "Loss:  0.17574439942836761\n",
      "Loss:  0.16468973457813263\n",
      "Loss:  0.19098332524299622\n",
      "Loss:  0.16849005222320557\n",
      "Loss:  0.17334696650505066\n",
      "0.8038325769036813 0.8730879139351151\n",
      "Loss:  0.17669923603534698\n",
      "Loss:  0.18605411052703857\n",
      "Loss:  0.18875449895858765\n",
      "Loss:  0.18033617734909058\n",
      "Loss:  0.16483688354492188\n",
      "0.7407967725668179 0.8730879139351151\n",
      "Loss:  0.22240854799747467\n",
      "Loss:  0.18347376585006714\n",
      "Loss:  0.1685992181301117\n",
      "Loss:  0.18183633685112\n",
      "Loss:  0.16501757502555847\n",
      "0.800302571860817 0.8730879139351151\n",
      "Loss:  0.17807015776634216\n",
      "Loss:  0.1832316666841507\n",
      "Loss:  0.16402025520801544\n",
      "Loss:  0.1884973794221878\n",
      "Loss:  0.1696513295173645\n",
      "0.746343923348462 0.8730879139351151\n",
      "Loss:  0.17023232579231262\n",
      "Loss:  0.16344521939754486\n",
      "Loss:  0.17231471836566925\n",
      "Loss:  0.17144381999969482\n",
      "Loss:  0.163688063621521\n",
      "0.7550848882165071 0.8730879139351151\n",
      "Loss:  0.19415371119976044\n",
      "Loss:  0.1752627193927765\n",
      "Loss:  0.16478368639945984\n",
      "Loss:  0.16638073325157166\n",
      "Loss:  0.18976205587387085\n",
      "0.7839973104723483 0.8730879139351151\n",
      "Loss:  0.17251525819301605\n",
      "Loss:  0.17021319270133972\n",
      "Loss:  0.16989967226982117\n",
      "Loss:  0.19058215618133545\n",
      "Loss:  0.21929338574409485\n",
      "0.7340729534375525 0.8730879139351151\n",
      "Loss:  0.16587525606155396\n",
      "Loss:  0.18183985352516174\n",
      "Loss:  0.17416049540042877\n",
      "Loss:  0.16398067772388458\n",
      "Loss:  0.16400578618049622\n",
      "0.7809715918641789 0.8730879139351151\n",
      "Loss:  0.16576987504959106\n",
      "Loss:  0.1758846640586853\n",
      "Loss:  0.16774329543113708\n",
      "Loss:  0.162183478474617\n",
      "Loss:  0.17741607129573822\n",
      "0.7639939485627836 0.8730879139351151\n",
      "Loss:  0.16340002417564392\n",
      "Loss:  0.16421064734458923\n",
      "Loss:  0.16689585149288177\n",
      "Loss:  0.16217246651649475\n",
      "Loss:  0.1829976737499237\n",
      "0.7912254160363087 0.8730879139351151\n",
      "Loss:  0.2210673689842224\n",
      "Loss:  0.16480804979801178\n",
      "Loss:  0.16602268815040588\n",
      "Loss:  0.18006348609924316\n",
      "Loss:  0.1684563159942627\n",
      "0.7288619936123719 0.8730879139351151\n",
      "Loss:  0.16767361760139465\n",
      "Loss:  0.18747302889823914\n",
      "Loss:  0.20987960696220398\n",
      "Loss:  0.17995673418045044\n",
      "Loss:  0.16411177814006805\n",
      "0.7806354009077155 0.8730879139351151\n",
      "Loss:  0.17906171083450317\n",
      "Loss:  0.1652667373418808\n",
      "Loss:  0.16361728310585022\n",
      "Loss:  0.17880749702453613\n",
      "Loss:  0.1755787432193756\n",
      "0.7596234661287611 0.8730879139351151\n",
      "Loss:  0.17529836297035217\n",
      "Loss:  0.16472505033016205\n",
      "Loss:  0.18983013927936554\n",
      "Loss:  0.1852932572364807\n",
      "Loss:  0.16588878631591797\n",
      "0.7086905362245755 0.8730879139351151\n",
      "Loss:  0.17952562868595123\n",
      "Loss:  0.19746509194374084\n",
      "Loss:  0.22932547330856323\n",
      "Loss:  0.20135647058486938\n",
      "Loss:  0.167472705245018\n",
      "0.764330139519247 0.8730879139351151\n",
      "Loss:  0.1767674684524536\n",
      "Loss:  0.20237532258033752\n",
      "Loss:  0.1651242971420288\n",
      "Loss:  0.16312846541404724\n",
      "Loss:  0.16822156310081482\n",
      "0.7631534711716254 0.8730879139351151\n",
      "Loss:  0.16405415534973145\n",
      "Loss:  0.16372841596603394\n"
     ]
    }
   ],
   "source": [
    "from optimization import BERTAdam\n",
    "\n",
    "\n",
    "d2vmodel = DialogueTransformer().to(torch.device(\"mps\"))\n",
    "\n",
    "transformer = d2vmodel.bert[0].auto_model\n",
    "param_optimizer = list(transformer.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}]\n",
    "optimizer = BERTAdam(optimizer_grouped_parameters, lr=1e-5, warmup=0.1, t_total=line_statistics(\"datasets/doc2dial/train.tsv\"))\n",
    "\n",
    "res = EvaluationResult()\n",
    "\n",
    "steps = 0 \n",
    "d2vmodel.train()\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = tuple(t.to(torch.device(\"mps\")) for t in batch)\n",
    "        output_dict = d2vmodel(batch, strategy='mean_by_role')\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "        loss = loss / 1.0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if steps % 20 == 0: \n",
    "            print(\"Loss: \", loss.item())\n",
    "        if steps % 100 == 0:\n",
    "            new_res = eval(d2vmodel)\n",
    "            print(new_res.acc, res.acc)\n",
    "            if new_res > res:\n",
    "                print(\"new best: \", new_res.acc)\n",
    "                torch.save(d2vmodel.state_dict(), \"dial2vec_model.bin\")\n",
    "                res = new_res\n",
    "\n",
    "\n",
    "            d2vmodel.train()\n",
    "\n",
    "        steps += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dial2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
